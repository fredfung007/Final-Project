\section{Results}
\input{data/resultfigure}
Observe that with the exponential loss, Deepboost has a smaller test error than Adaboost, which is in accordance with Cortes et al.’s work\cite{cortes2014deep}.
Noise (realistic), in general, does not affect the boosting algorithms harshly, which is different from Dietterich’s work\cite{dietterich2000experimental}.
As is reported from Cortes et al. it is difficult to obtain statistically significant results for small datasets.
Therefore, the level of the testing error at approximately less than 10\% is acceptable.

Figure \ref{adaiono} shows the training error and test error of Adaboost of the original ionosphere dataset,
while figure \ref{adaionon5} to \ref{adaionon20} shows the training error and test error with the introduced noise of
corresponding level of 5\%, 10\% and 20\%. On the other hand, figure \ref{adaIonoDeep} to \ref{adaIonoDeepn20} shows the training
and test error of Deepboost on those datasets with noise.

In figure \ref{adaionon5} to \ref{adaionon20} and figure \ref{adaIonoDeep} to \ref{adaIonoDeepn20},
with the increase of the noise, the training error is increased.
An intuitive explanation would be that the training error contains many of the noise that is being introduced.
These noise are distributed near the sample whose margin are relatively small.
Therefore, these noise would be intuitively difficult to classify correctly by boosting.
