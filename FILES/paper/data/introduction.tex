\section{Introduction}
The fundamental idea of ensemble methods is to construct a combination of weak base classifiers that are diverse and result in a high accuracy.
Multiple ensemble methods, including boosting\cite{}, bagging\cite{}, and decision tree ensemble\cite{}, are being introduced in the past 20 years.
Boosting algorithms took a significant place in ensemble methods. Adaboost\cite{freund1997decision} and the recently introduced Deepboost\cite{cortes2014deep}
are typical boosting algorithms with a good experimental result without overfitting the training set. They both have good theoretical learning bound
and benefit directly from minimizing the learning bound. However, the experimental robustness of these algorithms have not been tested before.
