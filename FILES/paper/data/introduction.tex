\section{Introduction}
The fundamental idea of ensemble methods is to construct a combination of diverse weak base classifiers and result in a high accuracy.
Multiple ensemble methods, including boosting\cite{freund1996experiments}, bagging\cite{breiman1996bagging}, and decision tree ensemble\cite{banfield2007comparison}, are being introduced in the past 20 years.
Boosting algorithms took a significant place in ensemble methods. Adaboost\cite{freund1997decision} and the recently introduced Deepboost\cite{cortes2014deep}
are well developed boosting algorithms which have good theoretical learning bound. Serval experiments shows that AdaBoost seems not overfitting the training set.

Boosting algorithms maintains a set of weights over the original training set $S$, and adjust these weights for each iteration.
They utilize the base classifiers and create a combination of these classifiers with a complex classifier that typically has a good performance.
Boosting increases weight of samples that are mislabeled by the base classifier and decreases weight of samples that are correctly labeled during each iteration.
Therefore, the algorithm will keep focus on the misclassified samples.
As we shall discuss later, noise is typically distributed densely near the misclassified samples.
Adaboost has been shown to be very effective in practical\cite{quinlan1996bagging}.
Since Adaboost is a special case of Deepboost by setting $\lambda=0$ and $\beta=0$, Deepboost will always out performs Adaboost.
Therefore, both of these boosting algorithm will have a good performance in practical.
However, the experimental robustness of these algorithms have not been tested before.

Since it is not very realistic that real word noise happens randomly as showed in Dietterich's work\cite{dietterich2000experimental}. His conclusion that the accuracy of AdaBoost is severely affected by the noise might not be persuasive.
By introducing a more realistic approach of generating noise, we showed that both AdaBoost and DeepBoost doesn't overfit the noise. Finally, an explanation of the results is given based on the theoretical learning bound from both algorithms.
